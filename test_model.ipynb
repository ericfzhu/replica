{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# class FullSizeTestDataset(Dataset):\n",
    "#     def __init__(self, directory):\n",
    "#         self.directory = directory\n",
    "#         self.image_files = [os.path.join(directory, f) for f in os.listdir(directory)]\n",
    "#         self.transform = transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             # Add any other transformations here\n",
    "#         ])\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_files)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path = self.image_files[idx]\n",
    "#         img = Image.open(img_path)\n",
    "#         img = img.resize((2048, 1536))  # Resize if necessary to handle edge cases\n",
    "#         patches = self.image_to_patches(img)\n",
    "#         return patches\n",
    "\n",
    "#     def image_to_patches(self, img):\n",
    "#         # Assuming img is a PIL image\n",
    "#         patches = []\n",
    "#         for i in range(0, img.width, 100):\n",
    "#             for j in range(0, img.height, 100):\n",
    "#                 # Handle edge cases for the last row and column\n",
    "#                 width, height = min(100, img.width - i), min(100, img.height - j)\n",
    "#                 patch = img.crop((i, j, i+width, j+height))\n",
    "#                 patch = self.transform(patch)\n",
    "#                 patches.append(patch)\n",
    "#         return torch.stack(patches)  # Return a tensor of all patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your model\n",
    "import torch\n",
    "import numpy as np\n",
    "from model import ResNeXt\n",
    "from dataloader import get_dataloaders\n",
    "from torchvision.utils import save_image\n",
    "model = ResNeXt().to('cuda')\n",
    "epoch = 10000\n",
    "\n",
    "checkpoint = torch.load(f'checkpoints/model_epoch_{epoch}.pth')\n",
    "model.load_state_dict(checkpoint['generator_state_dict'])\n",
    "\n",
    "# Prepare DataLoader\n",
    "batch_size = 50\n",
    "sample_size = 100\n",
    "train_loader, test_loader = get_dataloaders(batch_size=batch_size, sample_size=sample_size)\n",
    "# test_dataset = FullSizeTestDataset('data/iphone/test_data/full_size_test_images')\n",
    "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        real, target = data\n",
    "        real = real.to('cuda')\n",
    "        target = target.to('cuda')\n",
    "        output = model(real)\n",
    "        comparison = torch.cat((real[0], output[0]), dim=2)\n",
    "        save_image(comparison, f'images/output_{i}.jpg')\n",
    "        if i == 4:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¶ž'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(sum(range(ord(min(str(not()))))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSLR-ResNeXt-sCYE6uU5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
