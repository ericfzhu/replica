## Introduction
The original breakout convolutional neural network (CNN) introduced by Alex Krizhevsky et al. in 2012, which achieved SOTA top-1 and top-5 error rates of 37.5% and 17.0% on the 2010 ImageNet dataset respectively. This model featured a much deeper neural network architecture, the use of ReLU (Rectified Linear Unit) activation functions, and a dropout regularization technique to prevent overfitting.

## Architecture
ReLU activations ($f(x) = \max(0, x)$) were used in favor of the standard tanh activation function ($f(x) = \tanh(x)$), as the ReLU-based models trained several times faster than the tanh-based models.


## Training


## Results

WIP